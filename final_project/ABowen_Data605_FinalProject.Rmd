---
title: 'DATA605: Final Project'
author: "Andrew Bowen"
date: "2023-04-30"
output: html_document
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(hexbin)
```


## Problem 1
```{r}
set.seed(1234)
```   


```{r}
n <- 5
lambda <- 8

X <- rgamma(1:10000, shape=n, scale=1/lambda)
```


Calculating our sum of exponential distributions: `Y` and `Z`:
```{r}
Y <- 0
for (i in 1:n){
  Y <- Y + rexp(1:10000, rate=lambda)
}

Z <- rexp(1:10000, rate=lambda)
```


#### Expected value and variance of our PDFs
```{r}
print(mean(X))
print(mean(Y))
print(mean(Z))
```

```{r}
print(var(X))
print(var(Y))
print(var(Z))
```

```{r}
qplot(Z)
```

### Question 1b
Using calculus, calculate the expected value and variance of the Gamma pdf ($X$).  Using the moment generating function for exponentials, calculate the expected value of the single exponential (Z) and the sum of exponentials (Y)

#### Gamma Distirbution 
For the gamma distribution, the pdf is 

\begin{aligned}
  f(x; \alpha, \beta) = \frac{x^{\alpha - 1}e^{-\beta x}\beta^{\alpha}}{\Gamma(\alpha)} \,dx
\end{aligned}

where $\Gamma(\alpha) = (\alpha - 1)!$. We can find the expected value:

\begin{aligned}
  E(X) = \int_0^{\infty} x \cdot  \frac{x^{\alpha - 1}e^{-\beta x}\beta^{\alpha}}{\Gamma(\alpha)} \,dx
\end{aligned}\newline
       = \int_0^{\infty} \frac{\alpha}{\beta}\frac{\beta^{\alpha + 1}}{\Gamma(\alpha + 1)}x^{(\alpha + 1) - 1}e^{-\beta x} \,dx
\end{aligned}

We got to the step above using the fact that $\Gamma(a + 1) = a\Gamma(a)$. W ecan pull out our constants above and are left with the definition of the gamma distribution, which integrates to 1 over $(o, \infty)$

\begin{aligned}
  E(X) = \frac{\alpha}{\beta}\int_0^{\infty} \frac{\beta^{\alpha + 1}}{\Gamma(\alpha + 1)}x^{(\alpha + 1) - 1}e^{-\beta x} \,dx
       = \frac{\alpha}{\beta}
\end{aligned}

#### Moment-generating function for exponentials
Let's define our moment-generating function for the exponential distribution:

\begin{aligned}
  M_Z(t) = \frac{1}{1 - \frac{t}{\lambda}}, t < \lambda
\end{aligned}

We know the moment generating function of the exponential distribution, so we just need to take its first derivative and evaluate at $t=0$
\begin{aligned}
  \frac{\,dM}{\,dt} = \frac{d}{\,dt}(1 - \frac{t}{\lambda})^{-1} = \frac{1}{\lambda(1 - \frac{t}{\lambda})^2}
\end{aligned}

When we evaluate the above at $t=0$, we get $1/\lambda$, roughly in line with what we expect from our simulated data ($1/8 = 0.125$)
```{r}
mean(Z)
```

For the sum of exponentials (an [Eralng Distribution](https://en.wikipedia.org/wiki/Erlang_distribution)), we first need to figure out the moment generating function for the sum of exponentially-distributed random variables. We can use the property of moment generating functions that the sum of independent random variables produces a *product* moment generating function of the input variables $M_{X + Y}(t) = M_X(t)M_Y(t)$



\begin{aligned}
  M_Y(t) = (\frac{1}{1 - \frac{t}{\lambda}})^n
\end{aligned}

We can take the first derivative of this expression w.r.t $t$ and then evaluate at $t=0$ to find the expected value for our random variable $Y$

\begin{aligned}
  M_Y(t)\rvert_{t=0} = \frac{d}{\,dt} (\frac{1}{1 - \frac{t}{\lambda}})^n\rvert_{t=0} \newline
                     = n(\frac{1}{1 - \frac{t}{\lambda}})(\frac{-1}{\lambda})\frac{-1}{1 - \frac{t}{\lambda}} \rvert_{t=0}\newline
                     = n(1)\frac{1}{\lambda}(1) = \frac{n}{\lambda} 
\end{aligned}
This predicted value is pretty close to our simulated data mean
```{r}
print(n / lambda)

print(mean(Y))
```

### Question 1c
$P(Z > \lambda | Z > \lambda /2)$

We can rewrite this as $P(Z > λ/2 ∩ Z > λ) / P(Z > λ/2)$. In this case, the numerator can be rewritten as $P(Z > λ/2 ∩ Z > λ) = P(Z > λ)$. Since wee can treat these as independent, we can multiply them separately $P(Z > λ/2)$ & $P(Z > λ)$. 

First, let's find $P(Z > λ/2)$:

\begin{aligned}
  P(Z > λ/2) = 1 - P(Z \le \lambda) = 1 - F(\lambda)\newline
             = 1 - (1 - e^{-λλ}) = e^{-λ}
\end{aligned}

We can use the cumulative distribution function for the exponential distribution $F(x) = 1 - e^{-\lambda x}$.

Similarly, we can find $P(Z > λ/2) = 1 - F(λ/2) = 1 - (1 - e^(-λλ/2)) = e^(-λ/2)$. Dividing these, we get $e^{-\lambda / 2}$. Which we can plug in our value fo 8 to find the probability:
```{r}
(prob1c <- exp(-lambda / 2))
```


#### Question 1d
Similar to above, we can re-write the expression as $P(Z > 2λ) / P(Z > λ)$.

\begin{aligned}
  P(Z > 2\lambda) = 1 - (1 - e^{-\lambda2\lambda}) = e^{\lambda}
  P(Z > λ) = 1 - (1 - e^-{\lambda\lambda}) = e^{-\lambda}
\end{aligned}

These two expressions divided by each other give a probability of 1. This makes sense given that Z must be greater than $\lambda$ to be greater than $2\lambda$.

#### Question 1e
$P(Z>3λ | Z> λ)$

TODO: finish this CDF manipulation


### Question 2
*Loosely investigate whether P(YZ) = P(Y) P(Z) by building a table with quartiles and evaluating the marginal and joint probabilities.*
First, let's get the different quartiles of our simulated values for Y and Z
```{r quartile-y}
quartiles_to_keep <- c("25%", "50%", "75%", "100%")
(qY <- quantile(Y)[quartiles_to_keep])
```


```{r quartile-z}
(qZ <- quantile(Z)[quartiles_to_keep])

```

Now let's matrix multiply our simulated datasets to get the possible pairwise products of `Y` and `Z`. This is our "sample space" over our simulated data. In other words, this 
```{r}
joint_probs <- as.data.frame(qY %*% t(qZ))
rownames(joint_probs) <- c("ZQ1", "ZQ2", "ZQ3", "ZQ4")
colnames(joint_probs) <- c("YQ1", "YQ2", "YQ3", "YQ4")

N <- sum(joint_probs)
(joint_probs <- joint_probs / N)
```

Now we need to calculate the marginal probabilities (in an extra row and column) by summing our rows and then summing our columns, and then dividing our cell values by the sum of our random variable products, to convert our cell values into probabilities.

We can verify that this is a valid marginal probability table by summing the rows and columns of our frequency table
```{r verification}
xx <- 0
yy <- 0

# sum rows to 1
for (i in 1:nrow(joint_probs)){xx <- xx + sum(joint_probs[i, ])}
# sum columns to 1
for (i in 1:nrow(joint_probs)){yy <- yy + sum(joint_probs[, i])}

# Check both sums are equal to 1
(xx == 1 & yy == 1)
```
